{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt; plt.rcParams[\"font.family\"] = \"Palatino Linotype\"\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import seaborn as sns\n",
    "import emoji\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from imojify import imojify\n",
    "import gdown\n",
    "\n",
    "## Pyspark modules\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "results_dir = \"<path_to_result_directory>\"\n",
    "plots_dir = os.path.join(results_dir, \"Plots\")\n",
    "\n",
    "# Download and unzip Steam3.zip to parse the raw reviews\n",
    "raw_reviews_url = \"https://drive.google.com/file/d/1oQaRv-7VOoAfhsCW4HDk3xUx5bzRJOf7/view?usp=sharing\"\n",
    "gdown.download(raw_reviews_url, \"Steam3.zip\", quiet=False, fuzzy=True)\n",
    "with ZipFile(\"Steam3.zip\", 'r') as zObject:  \n",
    "    zObject.extractall(path=\"<path>\")\n",
    "\n",
    "raw_reviews_dir = os.path.join(results_dir, \"<path_to_Steam3_directory>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataframe has already been saved, don't run again.\n",
    "createDf = False\n",
    "if createDf:\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(raw_reviews_dir):\n",
    "        for file in files:\n",
    "            if file.startswith('part-'):  \n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            record = json.loads(line)\n",
    "                            for key in record:\n",
    "                                if isinstance(record[key], str):\n",
    "                                    record[key] = record[key].strip()\n",
    "                                    record[key] = record[key].replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "                            data.append(record)\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.fillna(\"\")\n",
    "    df.to_csv(os.path.join(results_dir, 'DF_Streaming.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the streaming data saved to a dataframe earlier\n",
    "df_pandas_orig = pd.read_csv(\"DF_Streaming.csv\")\n",
    "# Remove NA values\n",
    "df_pandas = df_pandas_orig.dropna(inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session for DF processing\n",
    "# Set environment variables for Spark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "spark = SparkSession.builder.appName('Spark_Predictions').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: long (nullable = true)\n",
      " |-- app_id: long (nullable = true)\n",
      " |-- review_text: string (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n",
      "7111\n",
      "['review_id', 'app_id', 'review_text', 'label']\n"
     ]
    }
   ],
   "source": [
    "# Create PySpark DataFrame from Pandas\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "df_spark.printSchema()\n",
    "print(df_spark.count())\n",
    "print(df_spark.columns) # 'review_id', 'app_id', 'review_text', 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of label distribution\n",
    "\n",
    "label_counts = df_pandas[\"label\"].value_counts().sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,8), dpi=300)\n",
    "bars = ax.bar(label_counts.index, label_counts, color = [\"Red\", \"Green\"])\n",
    "plt.title(\"Distribution of Reviews\", fontsize=32, fontweight=\"bold\")\n",
    "plt.xlabel('Label', fontsize=24, fontweight=\"bold\")\n",
    "plt.ylabel('Number of Reviews', fontsize=24, fontweight=\"bold\")\n",
    "ax.set_xticks([0,1])\n",
    "plt.xticks(fontsize=18)\n",
    "ax.set_xticklabels([\"Downvote (0)\", \"Upvote (1)\"])\n",
    "plt.yticks(fontsize=18)\n",
    "plt.grid(axis='y', linestyle=\":\")\n",
    "ax.bar_label(bars, fontsize=20)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.savefig(os.path.join(plots_dir, \"Review_Distribution.jpg\"), bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent words in reviews:\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def word_counts(reviews):\n",
    "    word_freq = {}    \n",
    "    for review in reviews:\n",
    "        tokens = word_tokenize(review.lower())\n",
    "        for token in tokens:\n",
    "            if token in stop_words or len(token) <= 2 or token in [\"n't\"]:\n",
    "                continue\n",
    "            word_freq[token] = word_freq.get(token,0) + 1    \n",
    "    return sorted(word_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "## Reviews for each label\n",
    "upvoted_reviews = list(df_pandas[df_pandas[\"label\"] == 1][\"review_text\"])\n",
    "downvoted_reviews = list(df_pandas[df_pandas[\"label\"] == 0][\"review_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate bar plot for the most frequently used words\n",
    "def plotMostFrequentWords(reviews, filename, label):\n",
    "    freqs = word_counts(reviews)[:10]\n",
    "    words = [i[0] for i in freqs]\n",
    "    counts = [i[1] for i in freqs]\n",
    "\n",
    "    plt.figure(figsize=(18,8), dpi=300)\n",
    "    ax = sns.barplot(x=words, y=counts)\n",
    "\n",
    "    plt.title(\"Most Frequent Words Used in \" + label + \" Reviews\", fontsize=32, fontweight=\"bold\")\n",
    "    plt.ylabel('Count', fontsize=24, fontweight=\"bold\")\n",
    "    plt.xlabel('Word', fontsize=24, fontweight=\"bold\")\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.grid(axis='y', linestyle=\":\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "plotMostFrequentWords(upvoted_reviews, os.path.join(plots_dir, \"Frequent_Words_Upvoted.jpg\"), \"Upvoted\")\n",
    "plotMostFrequentWords(downvoted_reviews, os.path.join(plots_dir, \"Frequent_Words_Downvoted.jpg\"), \"Downvoted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get emojis from reviews\n",
    "def get_emojis(reviews):\n",
    "    return set([char for char in reviews if char in emoji.UNICODE_EMOJI['en']])\n",
    "\n",
    "def emoji_frequencies(reviews):    \n",
    "    emoji_freqs = {}\n",
    "    unique_emoji = get_emojis(reviews.lower())\n",
    "    tokens = word_tokenize(reviews)\n",
    "    for token in tokens:\n",
    "        if token in unique_emoji:\n",
    "            emoji_freqs[token] = emoji_freqs.get(token,0)+1    \n",
    "    return sorted(emoji_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "upvoted_reviews_joined = \" \".join([review for review in upvoted_reviews])\n",
    "emojis_upvoted = emoji_frequencies(upvoted_reviews_joined)\n",
    "print(\"Emojis in Upvoted Reviews\", emojis_upvoted)\n",
    "\n",
    "downvoted_reviews_joined = \" \".join([review for review in downvoted_reviews])\n",
    "emojis_downvoted = emoji_frequencies(downvoted_reviews_joined)\n",
    "print(\"Emojis in Downvoted Reviews\", emojis_downvoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEmojis(df_emojis, label, offset, fig_size, filename=\"\", downvote_yticks=False):\n",
    "    def offset_image(cords, emoji, ax):\n",
    "        img = plt.imread(imojify.get_img_path(emoji))\n",
    "        im = OffsetImage(img, zoom=0.12)\n",
    "        im.image.axes = ax\n",
    "        ab = AnnotationBbox(im, (cords[0], cords[1]),  frameon=False, pad=0)\n",
    "        ax.add_artist(ab)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=fig_size, dpi=300)\n",
    "    bars = ax.bar(range(len(df_emojis)), df_emojis.Count, width=0.5, align=\"center\")\n",
    "    ax.set_xticks(range(len(df_emojis)))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.tick_params(axis='x', which='major', pad=26)\n",
    "    ax.set_ylim((0, ax.get_ylim()[1]+10))\n",
    "\n",
    "    for i, e in enumerate(df_emojis.Emoji):\n",
    "        offset_image([i, df_emojis.Count[i]+offset], e, ax)\n",
    "    plt.title(\"Emojis in \" + label + \" Reviews\", fontsize=42, fontweight=\"bold\")\n",
    "    plt.ylabel('Count', fontsize=32, fontweight=\"bold\")\n",
    "    plt.xlabel('Emoji', fontsize=32, fontweight=\"bold\")\n",
    "    if downvote_yticks:\n",
    "        plt.yticks([0,1], fontsize=18)\n",
    "    else:\n",
    "        plt.yticks(fontsize=18)\n",
    "    plt.grid(axis='y', linestyle=\":\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "df_emojis_upvoted = pd.DataFrame({\"Emoji\": dict(emojis_upvoted).keys(), \"Count\": dict(emojis_upvoted).values()})\n",
    "df_emojis_downvoted = pd.DataFrame({\"Emoji\": dict(emojis_downvoted).keys(), \"Count\": dict(emojis_downvoted).values()})\n",
    "\n",
    "print(sum(dict(emojis_upvoted).values()), sum(dict(emojis_downvoted).values())) # 109, 4\n",
    "\n",
    "plotEmojis(df_emojis_upvoted, \"Upvoted\", 10, (24,8), os.path.join(plots_dir, \"Emojis_Upvoted_Reviews.jpg\"), False)\n",
    "plotEmojis(df_emojis_downvoted, \"Downvoted\", 2, (24,4), os.path.join(plots_dir, \"Emojis_Downvoted_Reviews.jpg\"), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into training and test sets\n",
    "training_data, test_data = df_spark.randomSplit([0.8, 0.2], seed = 100)\n",
    "print(\"Number of reviews in training data:\", training_data.count())\n",
    "print(\"Number of reviews in test data:\", test_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|review_id| app_id|         review_text|label|               words|            filtered|            features|       rawPrediction|         probability|prediction|\n",
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|124141594|1730650|UPDATE!!!: After ...|    1|[update!!!:, afte...|[update!!!:, patc...|(5479,[0,1,3,5,6,...|[-942.53567424640...|[0.99248081640494...|       0.0|\n",
      "|131364642|1269300|Fun concept, best...|    1|[fun, concept,, b...|[fun, concept,, b...|(5479,[3,31,72,23...|[-29.796736230375...|[0.04980778661868...|       1.0|\n",
      "|134329345|2202690|The game is very ...|    1|[the, game, is, v...|[game, fun,, firs...|(5479,[1,6,8,9,11...|[-243.47902462196...|[5.74579169533525...|       1.0|\n",
      "|135846672|1269300|really fun to pla...|    1|[really, fun, to,...|[really, fun, pla...|(5479,[3,5,9,718]...|[-26.697185293026...|[0.03517143016082...|       1.0|\n",
      "|137052249|2202690|a fun gladiator g...|    1|[a, fun, gladiato...|[fun, gladiator, ...|(5479,[1,3,6,13,1...|[-61.623189298543...|[0.01704064614298...|       1.0|\n",
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6007698064792045"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline with Naive Bayes Model\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = Tokenizer(inputCol=\"review_text\", outputCol=\"words\") # stop words\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(list(stop_words))\n",
    "count_vectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "\n",
    "nb_model = NaiveBayes(smoothing=1)\n",
    "pipeline_nb = Pipeline(stages=[tokenizer, stopwords_remover, count_vectors, nb_model])\n",
    "\n",
    "fitted_pipeline_nb = pipeline_nb.fit(training_data)\n",
    "predictions_nb = fitted_pipeline_nb.transform(test_data)\n",
    "predictions_nb.show(5)\n",
    "\n",
    "evaluator_nb = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "evaluator_nb.evaluate(predictions_nb) # 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|review_id| app_id|         review_text|label|               words|            filtered|            features|       rawPrediction|         probability|prediction|\n",
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|124141594|1730650|UPDATE!!!: After ...|    1|[update!!!:, afte...|[update!!!:, patc...|(5479,[0,1,3,5,6,...|[1.60330029427770...|[0.83247914148083...|       0.0|\n",
      "|131364642|1269300|Fun concept, best...|    1|[fun, concept,, b...|[fun, concept,, b...|(5479,[3,31,72,22...|[-1.3592463995703...|[0.20436280939831...|       1.0|\n",
      "|134329345|2202690|The game is very ...|    1|[the, game, is, v...|[game, fun,, firs...|(5479,[1,6,8,9,11...|[-2.1462889918074...|[0.10467851233992...|       1.0|\n",
      "|135846672|1269300|really fun to pla...|    1|[really, fun, to,...|[really, fun, pla...|(5479,[3,5,9,724]...|[-1.6465019854329...|[0.16158227685665...|       1.0|\n",
      "|137052249|2202690|a fun gladiator g...|    1|[a, fun, gladiato...|[fun, gladiator, ...|(5479,[1,3,6,13,1...|[-2.0187380832245...|[0.11724953882057...|       1.0|\n",
      "+---------+-------+--------------------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8878716870404028"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline with Logistic Regression Model\n",
    "lr_model = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0.0)\n",
    "pipeline_lr = Pipeline(stages=[tokenizer, stopwords_remover, count_vectors, lr_model])\n",
    "\n",
    "fitted_pipeline_lr = pipeline_lr.fit(training_data)\n",
    "predictions_lr = fitted_pipeline_lr.transform(test_data)\n",
    "predictions_lr.show(5)\n",
    "\n",
    "evaluator_lr = BinaryClassificationEvaluator(labelCol=\"label\")\n",
    "evaluator_lr.evaluate(predictions_lr) # 0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted model\n",
    "fitted_pipeline_lr.write().overwrite().save('Models/score_classifier_lr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
